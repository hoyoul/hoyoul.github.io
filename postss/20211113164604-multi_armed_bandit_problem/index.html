<!DOCTYPE html>
<html>
  <head><title>Multi_Armed_bandit_problem</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />

<link rel="shortcut icon" href="./img/favicon.ico" type="image/x-icon">
<link rel="icon" href="./img/favicon.ico" type="image/x-icon">    

<link rel="stylesheet" href="/css/main.css">

</head>
  <body><header>
  <a href="/" id="logo">
    <img src="http://braindump.frege2godel.me/img/mylogo.png" alt="holy frege">
    <h3><span>H</span>oly <span>F</span>rege's <span id="note">notes</span></h3>
  </a>
    <small>G.frege를 너무 사랑하는 holy가...</small>  
</header>

<div class="container">
  <div class="page">
    <h1 class="collapsed-title">Multi_Armed_bandit_problem</h1>    
      <div class="content">
	<a href="http://braindump.frege2godel.me/postss/20211113164604-multi_armed_bandit_problem/" alt="Multi_Armed_bandit_problem" class="permalink"><h1>Multi_Armed_bandit_problem</h1></a>      
	<div class="ox-hugo-toc toc has-section-numbers">
<div class="heading">Table of Contents</div>
<ul>
<li><span class="section-num">1</span> <a href="#%EC%B0%B8%EC%A1%B0-site">참조 site</a></li>
<li><span class="section-num">2</span> <a href="#multi-armed-bandit-problem">Multi_Armed_bandit_problem</a></li>
<li><span class="section-num">3</span> <a href="#exploration-vs-exploitation">exploration vs exploitation</a></li>
<li><span class="section-num">4</span> <a href="#multi-armed-bandit-scientific-definition">multi-armed-bandit Scientific definition</a></li>
<li><span class="section-num">5</span> <a href="#and-epsilon-greedy-algorithms">ε-Greedy Algorithms</a></li>
<li><span class="section-num">6</span> <a href="#d41d8c"></a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="참조-site"><span class="section-num">1</span> 참조 site</h2>
<p><a href="https://lilianweng.github.io/lil-log/tag/reinforcement-learning">https://lilianweng.github.io/lil-log/tag/reinforcement-learning</a>
<a href="https://github.com/LeeHyeJin91/Multi-Armed-Bandit">https://github.com/LeeHyeJin91/Multi-Armed-Bandit</a>
<a href="http://ailab.kaist.ac.kr/papers/pdfs/KCC20161.pdf">http://ailab.kaist.ac.kr/papers/pdfs/KCC20161.pdf</a>
<a href="https://wonwooddo.tistory.com/94?category=712065">https://wonwooddo.tistory.com/94?category=712065</a>
<a href="https://brunch.co.kr/@chris-song/62">https://brunch.co.kr/@chris-song/62</a>
<a href="https://m.blog.naver.com/nilsine11202/221912267111">https://m.blog.naver.com/nilsine11202/221912267111</a>
<a href="http://doc.mindscale.kr/km/data_mining/11.html">http://doc.mindscale.kr/km/data_mining/11.html</a>
<a href="https://assaeunji.github.io/bayesian/2021-01-30-mab/">https://assaeunji.github.io/bayesian/2021-01-30-mab/</a>
<a href="https://wwiiiii.tistory.com/15">https://wwiiiii.tistory.com/15</a>
<a href="https://yamalab.tistory.com/135">https://yamalab.tistory.com/135</a>
<a href="https://jyoondev.tistory.com/134?category=826957">https://jyoondev.tistory.com/134?category=826957</a>
<a href="https://jhk0530.medium.com/multi-armed-bandit-6005743a4856">https://jhk0530.medium.com/multi-armed-bandit-6005743a4856</a></p>
<h2 id="multi-armed-bandit-problem"><span class="section-num">2</span> Multi_Armed_bandit_problem</h2>
<ul>
<li>etymology: slot machine을 one armed bandit으로 부른다.</li>
<li>multi</li>
</ul>
<p>The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems.</p>
<h2 id="exploration-vs-exploitation"><span class="section-num">3</span> exploration vs exploitation</h2>
<ul>
<li>
<p>음식점 문제: exploration(새로운 음식점을 찾으러 돌아다님), exploitation(어느정도 잘하는 음식점을 알고 그 음식점만을 이용함)</p>
</li>
<li>
<p>슬롯머신(bandit) 문제
n개의 slotmachine이 있는 카지노에서 주어진 시간(H)에 최대 누적보상(highest long-term-reward)을 얻는 전략에 관한 문제.</p>
</li>
<li>
<p>exploration 과 exploitation
exploration: slot macine을 play하는것 -&gt; 새로운 정보를 모으기 위해서 slot machine을 play한다.
exploitation: slot machine을 play하는것 -&gt; 이미 알고 있는 정보를 바탕으로 best action을 취한다. 즉 reward가 많은 slot machine을 play한다. 예를들어서, 모든 slot machine의 확률분포를 알고 있다면, 확률 높은 machine을 선택해서 집중적으로 게임하는게 제일 좋은 선택일 수 있다. 하지만, 각각의 machine에 대한 확률분포를 모른다. 그러면 exploration을 해서 신뢰성이 있을 만큼의 확률을 계산하고, 그 결과를 바탕으로 exploitation을 해야 하는데, 시간적인 제약으로 인해 trade off가 발생한다. 즉 어느정도의 시간을 exploitation을 하는데 써야하고, 어느정도의 시간을 exploitation해야 하는지 balance를 맞추는 작업이 필요하다.</p>
<ol>
<li>카지노에 간다.</li>
<li>n개의 slot machine이 있다.</li>
<li>사용자입장에선 어떤 slot machine을 선택해야 할지 아무런 정보가 없다. (unknown probability)</li>
<li>각각의 기계는 reward에 대한 확률값을 가지고 있다고 가정한다. 예를들어서, 모든 기계는 Bernoulli의 확률 분포를 따른다고 하자. 그러면, 모든 기계는  θ 와 1-θ 라는 두개의확률값을 가지고 있게 되는데,</li>
<li>10개중 임의의 하나의 slot machine을 선택하고 H시간 계속한다. <strong>여기서 얻는 reward보다 더 큰 reward가 있는지 없는 지 알 수 없다.</strong></li>
</ol>
<p>[2] 두 번째 전략
<strong>4) 10개 slot machine 모두 선택해서, 10번씩 동작시키기로 한다. (exploration)</strong></p>
<ol>
<li>1번 slot machine은 5번 reward가 나왔다. 나머지는 모두 1번의 reward가 나왔다.</li>
</ol>
<p><strong>6-1) 나는 앞으로 1번 slot machine에서만 H시간동안 게임을 하기로 결정한다. (exploitation)</strong></p>
<p>[3] 세 번째 전략
<strong>6-2) 나는 앞으로 1번 slot machine에는 5번 게임하고(exploitation), 나머지는 1번씩 하겠다.(exploration)</strong></p>
</li>
<li>
<p>trade off
slot machine의 누적 reward를 최대화하기 위해서 어느정도의 exploration과 exploitation을 할것인가를 trade off 라고 한다.</p>
</li>
</ul>
<h2 id="multi-armed-bandit-scientific-definition"><span class="section-num">4</span> multi-armed-bandit Scientific definition</h2>
<ul>
<li>모든 machine의 reward 확률은 정의되어 있다.</li>
<li>매 t 마다, 하나의 machine을 선택하고 reward를 받는다.</li>
<li></li>
</ul>
<h2 id="and-epsilon-greedy-algorithms"><span class="section-num">5</span> ε-Greedy Algorithms</h2>
<h2 id="d41d8c"><span class="section-num">6</span> </h2>
      
      </div>
        
  
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
  

  

  </div>
</div>  

<script src="/js/URI.js" type="text/javascript"></script>
<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
