<!DOCTYPE html>
<html>
  <head><title>[ppt_chapter2] MDP와 벨만 방정식</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />

<link rel="shortcut icon" href="./img/favicon.ico" type="image/x-icon">
<link rel="icon" href="./img/favicon.ico" type="image/x-icon">    

<link rel="stylesheet" href="/css/main.css">

</head>
  <body><header>
  <a href="/" id="logo">
    <img src="http://braindump.frege2godel.me/img/mylogo.png" alt="holy frege">
    <h3><span>H</span>oly <span>F</span>rege's <span id="note">notes</span></h3>
  </a>
    <small>G.frege를 너무 사랑하는 holy가...</small>  
</header>

<div class="container">
  <div class="page">
    <h1 class="collapsed-title">[ppt_chapter2] MDP와 벨만 방정식</h1>    
      <div class="content">
	<a href="http://braindump.frege2godel.me/postss/20211221092543-ppt_chapter2_mdp%E1%84%8B%E1%85%AA_%E1%84%87%E1%85%A6%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB_%E1%84%87%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%89%E1%85%B5%E1%86%A8/" alt="[ppt_chapter2] MDP와 벨만 방정식" class="permalink"><h1>[ppt_chapter2] MDP와 벨만 방정식</h1></a>      
	<h2 id="2장-요약">2장 요약</h2>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/agent.png"
         alt="Figure 1: agent와 environment"/><figcaption>
            <p><span class="figure-number">Figure 1: </span>agent와 environment</p>
        </figcaption>
</figure>
 <br/></p>
<ul>
<li>agent가 환경과 상호작용하는 상황에서, 순차적 결정 문제가 발생한다. <br/></li>
<li>이 상황을 수학적으로 정의한 것이 MDP. <br/></li>
<li>agent가 결정하는 원리는 크기 비교다. 예상되는 가치(보상)의 크기 비교를 통해서 (행동을) 결정을 하게 된다. <br/></li>
<li>벨만 방정식은 가치(total discounted return의 기대값)를 재귀적으로 표현한것. <br/></li>
</ul>
<h2 id="mdp">MDP</h2>
<ul>
<li>MDP의 구성요소 <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/mdp1.jpeg"/>
</figure>
 <br/></p>
<h2 id="mdp-2">MDP 2</h2>
<ul>
<li>mdp의 구성요소들은 random하다. 그래서 확률변수로 표현한다. 확률변수는 사건을 입력으로 받아 수치를 return하는 function이다. <br/></li>
<li>확률 변수를 사용하지 않으면 수치화가 불가능 하다. <br/>
ex) 동전던지기 - 앞면이 나올때, 혹은 뒷면일때 기대값을 나타낼 수 없다. 크기를 나타낼 수 없다. <br/></li>
</ul>
<h2 id="상태">상태</h2>
<ul>
<li>Agent가 관찰 가능한 상태의 <strong>집합</strong>. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/state1.jpeg" width="600px" height="450px"/>
</figure>
 <br/></p>
<h2 id="행동">행동</h2>
<ul>
<li>agent가 상태 S에서 할수 있는 가능한 <strong>행동의 집합</strong>. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/action.jpeg"
         alt="Figure 2: action"/><figcaption>
            <p><span class="figure-number">Figure 2: </span>action</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="보상함수">보상함수</h2>
<ul>
<li>시간 t에서 action을 수행을 했을 때 환경이 주는 정보의 값(크기) <br/></li>
<li>기대값으로 표현한다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/reward1.jpeg"
         alt="Figure 3: reward1"/><figcaption>
            <p><span class="figure-number">Figure 3: </span>reward1</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="보상함수2">보상함수2</h2>
<ul>
<li>기대값을 사용하는 이유? <br/>
(1) 비교를 위해선 값이 필요하다. 확률은 크기를 나타낼 수 없다. <br/>
(2)  동일한 상태에서 동일한 action을 수행했다고 해서 reward가 동일하지 않다. random하다. <br/>
(3)  return(reward의 total discounted reward)에서도 기대값을 사용하는데, 조금 의미가 다르다.  return에서는  가능한 action들에 따른 path들의 평균을 구하고자 하는 것이고, reward function 은 동일한 상태 S와 action이 주어져도 다른 값을 가질수 있기 때문에 기대값을 사용한다. <br/></li>
</ul>
<h2 id="상태변환-확률">상태변환 확률</h2>
<ul>
<li>상태 s에서 행동 a를 수행했을 때, 다른 상태 s&rsquo;에 도달할 확률. <br/></li>
<li>환경으로부터 주어지며,  agent가 action을 취한다고 해서 반드시 예상되는 상태에 도달할 수 없다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/state_transition1.jpeg"/>
</figure>
 <br/></p>
<h2 id="할인율">할인율</h2>
<ul>
<li>가치함수에서 누적보상을 사용하는데, 시간에 따른 보상의 차이를 두기 위해서 생겨난 개념. <br/></li>
</ul>
<!--listend-->
<ul>
<li>discount를 적용하지 않으면 아래는 동일한 값을 갖는다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/discount1.jpeg" width="400px" height="300px"/>
</figure>
 <br/></p>
<h2 id="할인율2">할인율2</h2>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/discount2.jpeg"
         alt="Figure 4: discount2" width="400px" height="300px"/><figcaption>
            <p><span class="figure-number">Figure 4: </span>discount2</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="정책-1">정책 1</h2>
<ul>
<li>agent 학습의 목표, 최적 정책을 얻는것. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/policy1.jpeg" width="600px" height="300px"/>
</figure>
 <br/></p>
<h2 id="정책-2">정책 2</h2>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/policy2.jpeg"
         alt="Figure 5: policy2" width="400px" height="350px"/><figcaption>
            <p><span class="figure-number">Figure 5: </span>policy2</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="가치함수1">가치함수1</h2>
<ul>
<li>상태의 가치를  return의 기대값으로 나타낸다. return은  total discounted rewards. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/return1.jpeg"/>
</figure>
 <br/></p>
<h2 id="가치함수2">가치함수2</h2>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/return2.jpeg"
         alt="Figure 6: return value"/><figcaption>
            <p><span class="figure-number">Figure 6: </span>return value</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="가치함수3">가치함수3</h2>
<ul>
<li>가치함수를 다음상태의 가치와의 관계로 표현할 수 있다. (벨만 기대 방정식) <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/value2.jpeg"/>
</figure>
 <br/></p>
<h2 id="가치함수4">가치함수4</h2>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/value1.jpeg"
         alt="Figure 7: value function" width="600px" height="300px"/><figcaption>
            <p><span class="figure-number">Figure 7: </span>value function</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="q함수">Q함수</h2>
<ul>
<li>상태S와 action a를 입력으로 받아, 가치를 return 하는 함수. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/q1.jpeg"/>
</figure>
 <br/></p>
<h2 id="q함수와-가치함수의-관계식">Q함수와 가치함수의 관계식</h2>
<ul>
<li>Q함수는 가치함수로 나타낼 수 있다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/q2.jpeg"/>
</figure>
 <br/></p>
<h2 id="q함수의-벨만-기대-방정식">Q함수의 벨만 기대 방정식</h2>
<ul>
<li>Q함수도 다음 Q함수를 사용해서 재귀적인 방식으로 나타낼수 있다. (Q함수의 벨만기대방정식) <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/q3.jpeg"/>
</figure>
 <br/></p>
<h2 id="벨만-기대-방정식">벨만 기대 방정식</h2>
<ul>
<li>벨만 기대 방정식은 재귀적으로 표현된다. <br/></li>
<li>기대값을 계산할 수 있게 하기 위해서, 식을 변형할 수 있다. Reward값은 시간 t, 상태 S에서 action a를 수행하고, 상태가 전이하면서 받는 보상이기 때문에, 정책과 state transition matrix 가 숨어져 있다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/bellman1.jpeg"/>
</figure>
 <br/></p>
<h2 id="벨만-최적-방정식">벨만 최적 방정식</h2>
<ul>
<li>최적(optimal)의 방정식은 최적의 정책을 수행했을때, 얻게 되는 누적보상을 계산할 수 있는 방정식이다. <br/></li>
</ul>
<p><a id="figure--"></a></p>
<p><figure><img src="./img/optimal1.jpeg"
         alt="Figure 8: 최적 방정식" width="400px" height="300px"/><figcaption>
            <p><span class="figure-number">Figure 8: </span>최적 방정식</p>
        </figcaption>
</figure>
 <br/></p>
<h2 id="요약">요약</h2>
<ul>
<li>agent가 특정상황에 직면했을 때 어떤 action을 판단할지를 결정하기 위해서 MDP문제로 만들고, 답을 찾는과정에  벨만 방정식을  사용한다. <br/></li>
<li>agent는 다음상태로 이동하기 전에, 다음상태의 가치를 비교를 통해서 결정할 수도 있고, 다음상태와 주어진 action에 따른 가치 비교를 통해서 결정할 수도 있다. <br/></li>
</ul>
      
      </div>
        
  
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
  

  

  </div>
</div>  

<script src="/js/URI.js" type="text/javascript"></script>
<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
