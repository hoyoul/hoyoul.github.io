<!DOCTYPE html>
<html>
  <head><title>[chapter6] Actor-Critic</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />

<link rel="shortcut icon" href="./img/favicon.ico" type="image/x-icon">
<link rel="icon" href="./img/favicon.ico" type="image/x-icon">    

<link rel="stylesheet" href="/css/main.css">

</head>
  <body><header>
  <a href="/" id="logo">
    <img src="http://braindump.frege2godel.me/img/mylogo.png" alt="holy frege">
    <h3><span>H</span>oly <span>F</span>rege's <span id="note">notes</span></h3>
  </a>
    <small>G.frege를 너무 사랑하는 holy가...</small>  
</header>

<div class="container">
  <div class="page">
    <h1 class="collapsed-title">[chapter6] Actor-Critic</h1>    
      <div class="content">
	<a href="http://braindump.frege2godel.me/postss/20220219001950-test2/" alt="[chapter6] Actor-Critic" class="permalink"><h1>[chapter6] Actor-Critic</h1></a>      
	<h2 id="지금까지-공부한-것">지금까지 공부한 것</h2>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/summary.png"
         alt="Figure 1: summary"/><figcaption>
            <p><span class="figure-number">Figure 1: </span>summary</p>
        </figcaption>
</figure>

<h2 id="policy-iteration-review">Policy Iteration review</h2>
<ul>
<li>Actor &amp; Critic방식은 Policy Iteration방식을 기반으로 한다.</li>
<li>Policy iteration으로 볼때 Actor는 policy table을 따르고, Critic은
value table을 따르는 것과 동일하게 볼수 있다.</li>
<li>따라서Policy iteration의 동작을 review할 필요가 있다.</li>
</ul>
<h3 id="policy">Policy</h3>
<figure><img src="./img/chapter6/policy.jpeg"/>
</figure>

<h3 id="policy-function">policy function</h3>
<ul>
<li>입력: 상태를 입력받는다.</li>
<li>출력: random하게 출력( policy 확률분포를 따른다.)</li>
</ul>
<h3 id="동작방식">동작방식</h3>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policy_progress.png" width="400px"/>
</figure>

<ul>
<li>agent는 policy function을 통해서 이동한다.</li>
<li>policy function에는 state와 action에 따른 확률분포를 따라
action이 정해진다.</li>
<li>예를들면, s0의 상태에서 policy 값(동:20%, 서:20%,
북:40%,남:20%)에 따라 random하게 선택되기 때문에 북쪽으로 이동할
확률이 높다.</li>
<li>매번 확률적으로 action을 선택을 한다. value iteration는
greedy하게 이동했고, local maximum에 빠지지 않기 위해서
epsilon-greedy방식을 썼지만, policy iteration은 epsilon-greedy를
쓸필요가 없다. 랜덤하게 선택하기 때문이다.</li>
</ul>
<h3 id="update방식">update방식</h3>
<ul>
<li>policy iteration</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policy2.jpeg"
         alt="Figure 3: Policy iteration 과정" width="400px"/><figcaption>
            <p><span class="figure-number">Figure 3: </span>Policy iteration 과정</p>
        </figcaption>
</figure>

<h3 id="정책-평가">정책 평가</h3>
<ul>
<li>가치함수 update: 모든 상태의 value를 다 구한다.</li>
</ul>
<h3 id="정책-발전">정책 발전</h3>
<ul>
<li>정책은 모든 state와 action에 대한 확률값으로 이루어져
있다.</li>
<li>updade는 Q값에 대한 확률값으로 계산한다.</li>
</ul>
<h3 id="정책-발전2">정책 발전2</h3>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policy_update.png" width="400px"/>
</figure>

<ul>
<li>현 상태에서 Q값들을 구한다.</li>
<li>Q값중에서 가장큰 값의 action들을 모은다.</li>
<li>예를 들어, S7에서 S8로 갈때 value가 10, S7에서 S6으로 갈때
value가 10, S7에서 S2로 갈때 5, S7에서 S10으로 갈때 value가 5라고
하면, 가장 큰 S6,S8이다.</li>
<li>각 action의 확률을 구해서 policy table을 udpate한다.</li>
</ul>
<h2 id="policy-gradient">Policy Gradient</h2>
<ul>
<li>Policy Gradient는 Actor &amp; Critic이나 Reinforce방식의 정책신경망의
목적함수를 나타낸다.</li>
<li>우리의 goal: 정책신경망을 만들고, objective function을 정의하고,
Policy gradietn방식을 사용해서 정책신경망을 update하고자 한다.</li>
</ul>
<h3 id="overview">Overview</h3>
<ul>
<li>DNN(Deep Neural Network)을 케라스로 만들때 다음과 같은 과정을 취했다.</li>
<li>(다음) Model network 만들기 -&gt; Objective function -&gt; Optimization</li>
<li>우리의 정책신경망은 다음과 같이 표현할 수 있다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policy_network.png" width="400px"/>
</figure>

<ul>
<li>objective function의 목적은 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/objectivefunction1.png" width="400px"/>
</figure>

<ul>
<li>여기서 J()는 무엇인가?</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/jvalue.png" width="400px"/>
</figure>

<ul>
<li>objective function은 최대값을 찾아가는 Gradient Ascent방식이다.</li>
<li>optimization하는 과정은 정책신경망을 update하는 것을 의미한다. 이
과정은 다음과 같이 나타낼 수 있다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/optimization1.png" width="400px"/>
</figure>

<h2 id="policy-gradient-수식-전개">Policy Gradient 수식 전개</h2>
<ul>
<li>정책 신경망에서 update를 하기 위해서는 목표함수를 미분값을 구해야 한다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policygrad_math1.jpeg" width="400px"/>
</figure>

<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policygrad_math2.jpeg" width="400px"/>
</figure>

<ul>
<li>이 식을 사용하면, 우리는 정책신경망 네트웍을 만들고, update가 가능하다.</li>
<li>그런데, 정책신경망에서는, Q값을 따로 유지하고 있지 않다. 이것을
대체할 무언가가 필요하다.</li>
<li>정책신경망으로 grid world를 이동할때 sample들을 기록하고 이 값을
학습(update)에 사용하는 방식을 쓴다.</li>
</ul>
<h2 id="reinforce-알고리즘">Reinforce 알고리즘</h2>
<ul>
<li>Renforce 방식도 정책신경망을 사용한다.</li>
<li>정책신경망을 update할 때 문제가 되는 Q값을 Reinforce에서는
몬테카를로 방식을 사용한다.</li>
<li>즉 episode를 마칠때까지 sample들을 저장(기록)한다.</li>
<li>저장된 sample들로 부터 return값을 구할 수 있다.</li>
<li>episode가 끝나면 정책망도 update한다.</li>
</ul>
<h2 id="reinforce-알고리즘의-오류함수">Reinforce 알고리즘의  오류함수</h2>
<ul>
<li>MSE(Mean Squared Error)방식은 정답과 오류의 차이를 통해서 update했다.</li>
<li>Reinforce 알고리즘의 update는 cross entropy를 사용한다.</li>
<li>cross entropy는 정답과 오답이 얼만큼 비슷한가를 의미한다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/crossentropy.png" width="400px"/>
</figure>

<ul>
<li>정답에 해당하는 return값과 정책망을 통과한 값이 거의 같다면, cross
entropy는 작아지고, 차이가 크다면 cross entropy는 커진다.</li>
</ul>
<h2 id="reinforce의-문제">Reinforce의 문제</h2>
<ul>
<li>Reinforce가 정책망을 update하는 데 있어서, Q값을 return값으로
대체한다. 여기서 문제가 있을 수 있다.</li>
<li>return값은 episode별로 다양한 route를 갖는데, 엄청나게 긴 경로를
갖는 route가 많이 생길수 있다. 이런것은 평균을 볼때, 분산의 크기를
키우는 경향이 있다.</li>
<li>정책신경망을 update할 때, 분산이 큰 return값을 사용하면 update가
비효율적일 수 있다.</li>
</ul>
<h2 id="actor-critic-알고리즘">Actor-Critic 알고리즘</h2>
<ul>
<li>Actor-Critic 방식은 정책 iteration방식과 동일하다.</li>
<li>정책개선은 Actor로, 정책 발전은 Critic으로 보면된다.</li>
<li>Reinforce방식에선 정책신경망 1개를 사용했다. 그리고 update시에 Q값
대신 sample에 대한 return값을 사용했다.</li>
<li>Actor-Critic 방식에선 신경망 2개를 사용한다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/critic_actic1.png" width="400px"/>
</figure>

<h2 id="actor-critic-알고리즘-objective-function--loss-function">Actor-Critic 알고리즘 - Objective function(loss function)</h2>
<ul>
<li>Actor-Critic Algorithm의 object function은 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/critic_actic_objective_function.png" width="400px"/>
</figure>

<ul>
<li>여기서 문제가 되는 것은 Q함수의 값에 따라 오류함수의 변화가 크다는 것이다.</li>
<li>Q함수의 변화율을 줄이기 위해서  base line을 사용한다.</li>
<li>baseline으로 Actor-Critic에선 value함수를 사용한다고 한다.</li>
<li>Q함수에서 baseline을 빼주는 함수를 Advantage함수라고 한다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/advantage.png" width="400px"/>
</figure>

<ul>
<li>Advantage의 함수에서 사용되는 Q값과 V값을 위해서, 2개의 신경망을
사용하는것은 비효율적이다. Value function만 신경망으로 쓰겠다.</li>
<li>Q값을 value function형태로 변경할 수 있다. Advantage함수를 변형하면 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/advantage2.png" width="400px"/>
</figure>

<ul>
<li>objective function의 update식도 아래와 같이 구할 수 있다. upadate식은 미분값을 구하면 된다. 자동으로 해준다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/advantage3.png" width="400px"/>
</figure>

<h2 id="actor-critic-알고리즘-critic의-loss-function">Actor-Critic 알고리즘 - critic의 loss function</h2>
<ul>
<li>value function을 신경망으로 바꿨기 때문에 loss function의 정의가 필요하다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/critic_loss.png" width="400px"/>
</figure>

<ul>
<li>1-step이동할때 마다, 현재 상태, 다음상태, 보상을 사용해서 value 신경망을 update할 수 있다.</li>
</ul>
<h2 id="actor-critic-알고리즘-요약">Actor-Critic 알고리즘 요약</h2>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/a2c.png" width="400px"/>
</figure>

<ul>
<li>Actor-Critic은 crossentropy를 사용한다. 사용되는 object function에
Q값을 value 신경망이 대체하기 때문에 2개의 신경망을 사용한다.</li>
</ul>
<h2 id="actor-critic-소스분석--env-cartpole-v0">Actor-Critic 소스분석 (env-cartpole-v0)</h2>
<p>ps: <a href="https://github.com/openai/gym/wiki/CartPole-v0">https://github.com/openai/gym/wiki/CartPole-v0</a></p>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/cartpole.png"
         alt="Figure 11: cart pole" width="400px"/><figcaption>
            <p><span class="figure-number">Figure 11: </span>cart pole</p>
        </figcaption>
</figure>

<ul>
<li>목적: 막대기가 쓰러지지 않게하는게 목표</li>
</ul>
<h2 id="actor-critic-소스분석--state">Actor-Critic 소스분석 (state)</h2>
<ul>
<li>상태: 무한개의 상태공간을 갖는다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/cartpole3.png" width="400px"/>
</figure>

<ul>
<li>매순간 상태값(cart의 위치, cart의 속도, pole의 각도, pole의 각속도)이 agent에게 주어진다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/cartpole2.png" width="400px"/>
</figure>

<h2 id="actor-critic-소스분석--action">Actor-Critic 소스분석 (action)</h2>
<ul>
<li>action은 2개가 있다. 왼쪽으로 미는 경우, 오른쪽으로 미는 경우</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/cartpole4.png" width="400px"/>
</figure>

<h2 id="actor-critic-소스분석--reward">Actor-Critic 소스분석 (reward)</h2>
<ul>
<li>매 step마다 +1의 보상을 받는다. A2C 소스에서는 보상을 매 step 0.1을 준다.</li>
</ul>
<h2 id="actor-critic-소스분석">Actor-Critic 소스분석</h2>
<ul>
<li>실행과정은 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/source_anal1.png" width="400px"/>
</figure>

<h3 id="소스-분석">소스 분석</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">sys</span>
</span></span><span style="display:flex;"><span>sys<span style="color:#666">.</span>path<span style="color:#666">.</span>append(<span style="color:#4070a0">&#39;/Users/holy/opt/anaconda3/envs/keras_rl_3.7/lib/python3.7/site-packages&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">gym</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">pylab</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">tensorflow</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">tf</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">tensorflow.keras.layers</span> <span style="color:#007020;font-weight:bold">import</span> Dense
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">tensorflow.keras.optimizers</span> <span style="color:#007020;font-weight:bold">import</span> Adam
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">tensorflow.keras.initializers</span> <span style="color:#007020;font-weight:bold">import</span> RandomUniform
</span></span></code></pre></div><h4 id="main함수">Main함수</h4>
<ul>
<li>main함수에서는, 매시간 policy network의 출력에 따라 action을 수행한다. (1-step 이동)</li>
<li>1step 이동 후 환경으로부터 보상과 next state정보를 얻어온다.</li>
<li>받은 정보로 Agent class에 있는 policy network와 value network를  update한다.</li>
<li>update된 policy network의 출력에 따라 다시 action을 수행한다. (1-step 이동)</li>
<li>PS: wiki에 보면 reward는 1로 주지만, source에는 0.1을 준다. 이유는 모르겠다.</li>
</ul>
<!--listend-->
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">if</span> __name__ <span style="color:#666">==</span> <span style="color:#4070a0">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#60a0b0;font-style:italic"># CartPole-v1 환경, 최대 타임스텝 수가 500</span>
</span></span><span style="display:flex;"><span>    env <span style="color:#666">=</span> gym<span style="color:#666">.</span>make(<span style="color:#4070a0">&#39;CartPole-v1&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#60a0b0;font-style:italic"># 환경으로부터 상태와 행동의 크기를 받아옴</span>
</span></span><span style="display:flex;"><span>    state_size <span style="color:#666">=</span> env<span style="color:#666">.</span>observation_space<span style="color:#666">.</span>shape[<span style="color:#40a070">0</span>]
</span></span><span style="display:flex;"><span>    action_size <span style="color:#666">=</span> env<span style="color:#666">.</span>action_space<span style="color:#666">.</span>n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#60a0b0;font-style:italic"># 액터-크리틱(A2C) 에이전트 생성</span>
</span></span><span style="display:flex;"><span>    agent <span style="color:#666">=</span> A2CAgent(action_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    scores, episodes <span style="color:#666">=</span> [], []
</span></span><span style="display:flex;"><span>    score_avg <span style="color:#666">=</span> <span style="color:#40a070">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    num_episode <span style="color:#666">=</span> <span style="color:#40a070">1000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">for</span> e <span style="color:#007020;font-weight:bold">in</span> <span style="color:#007020">range</span>(num_episode):
</span></span><span style="display:flex;"><span>	done <span style="color:#666">=</span> <span style="color:#007020;font-weight:bold">False</span>
</span></span><span style="display:flex;"><span>	score <span style="color:#666">=</span> <span style="color:#40a070">0</span>
</span></span><span style="display:flex;"><span>	loss_list <span style="color:#666">=</span> []
</span></span><span style="display:flex;"><span>	state <span style="color:#666">=</span> env<span style="color:#666">.</span>reset()
</span></span><span style="display:flex;"><span>	state <span style="color:#666">=</span> np<span style="color:#666">.</span>reshape(state, [<span style="color:#40a070">1</span>, state_size])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#007020;font-weight:bold">while</span> <span style="color:#007020;font-weight:bold">not</span> done:
</span></span><span style="display:flex;"><span>	    <span style="color:#007020;font-weight:bold">if</span> agent<span style="color:#666">.</span>render:
</span></span><span style="display:flex;"><span>		env<span style="color:#666">.</span>render()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    action <span style="color:#666">=</span> agent<span style="color:#666">.</span>get_action(state)
</span></span><span style="display:flex;"><span>	    next_state, reward, done, info <span style="color:#666">=</span> env<span style="color:#666">.</span>step(action)
</span></span><span style="display:flex;"><span>	    next_state <span style="color:#666">=</span> np<span style="color:#666">.</span>reshape(next_state, [<span style="color:#40a070">1</span>, state_size])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#60a0b0;font-style:italic"># 타임스텝마다 보상 0.1, 에피소드가 중간에 끝나면 -1 보상</span>
</span></span><span style="display:flex;"><span>	    score <span style="color:#666">+=</span> reward
</span></span><span style="display:flex;"><span>	    reward <span style="color:#666">=</span> <span style="color:#40a070">0.1</span> <span style="color:#007020;font-weight:bold">if</span> <span style="color:#007020;font-weight:bold">not</span> done <span style="color:#007020;font-weight:bold">or</span> score <span style="color:#666">==</span> <span style="color:#40a070">500</span> <span style="color:#007020;font-weight:bold">else</span> <span style="color:#666">-</span><span style="color:#40a070">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#60a0b0;font-style:italic"># 매 타임스텝마다 학습</span>
</span></span><span style="display:flex;"><span>	    loss <span style="color:#666">=</span> agent<span style="color:#666">.</span>train_model(state, action, reward, next_state, done)
</span></span><span style="display:flex;"><span>	    loss_list<span style="color:#666">.</span>append(loss)
</span></span><span style="display:flex;"><span>	    state <span style="color:#666">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#007020;font-weight:bold">if</span> done:
</span></span><span style="display:flex;"><span>		<span style="color:#60a0b0;font-style:italic"># 에피소드마다 학습 결과 출력</span>
</span></span><span style="display:flex;"><span>		score_avg <span style="color:#666">=</span> <span style="color:#40a070">0.9</span> <span style="color:#666">*</span> score_avg <span style="color:#666">+</span> <span style="color:#40a070">0.1</span> <span style="color:#666">*</span> score <span style="color:#007020;font-weight:bold">if</span> score_avg <span style="color:#666">!=</span> <span style="color:#40a070">0</span> <span style="color:#007020;font-weight:bold">else</span> score
</span></span><span style="display:flex;"><span>		<span style="color:#007020">print</span>(<span style="color:#4070a0">&#34;episode: </span><span style="color:#70a0d0">{:3d}</span><span style="color:#4070a0"> | score avg: </span><span style="color:#70a0d0">{:3.2f}</span><span style="color:#4070a0"> | loss: </span><span style="color:#70a0d0">{:.3f}</span><span style="color:#4070a0">&#34;</span><span style="color:#666">.</span>format(
</span></span><span style="display:flex;"><span>		      e, score_avg, np<span style="color:#666">.</span>mean(loss_list)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#60a0b0;font-style:italic"># 에피소드마다 학습 결과 그래프로 저장</span>
</span></span><span style="display:flex;"><span>		scores<span style="color:#666">.</span>append(score_avg)
</span></span><span style="display:flex;"><span>		episodes<span style="color:#666">.</span>append(e)
</span></span><span style="display:flex;"><span>		pylab<span style="color:#666">.</span>plot(episodes, scores, <span style="color:#4070a0">&#39;b&#39;</span>)
</span></span><span style="display:flex;"><span>		pylab<span style="color:#666">.</span>xlabel(<span style="color:#4070a0">&#34;episode&#34;</span>)
</span></span><span style="display:flex;"><span>		pylab<span style="color:#666">.</span>ylabel(<span style="color:#4070a0">&#34;average score&#34;</span>)
</span></span><span style="display:flex;"><span>		pylab<span style="color:#666">.</span>savefig(<span style="color:#4070a0">&#34;./save_graph/graph.png&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#60a0b0;font-style:italic"># 이동 평균이 400 이상일 때 종료</span>
</span></span><span style="display:flex;"><span>		<span style="color:#007020;font-weight:bold">if</span> score_avg <span style="color:#666">&gt;</span> <span style="color:#40a070">400</span>:
</span></span><span style="display:flex;"><span>		    agent<span style="color:#666">.</span>model<span style="color:#666">.</span>save_weights(<span style="color:#4070a0">&#34;./save_model/model&#34;</span>, save_format<span style="color:#666">=</span><span style="color:#4070a0">&#34;tf&#34;</span>)
</span></span><span style="display:flex;"><span>		    sys<span style="color:#666">.</span>exit()
</span></span></code></pre></div><h4 id="agent">Agent</h4>
<ul>
<li>policy network의 loss function은 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/policy_nt.png" width="400px"/>
</figure>

<ul>
<li>정책 네트워크의 예측값에 log를 씌운 값 X advantage함수의 형태</li>
<li>value network의 loss function은 다음과 같다.</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/value_nt.png" width="400px"/>
</figure>

<ul>
<li>
<p>update목표에 해당하는 값에 stop_gradient를 사용해서 정책신경망이 update되는 것을 막는다.</p>
</li>
<li>
<p>loss함수를 하나로 만든다.(value network에 더 많은 가중치를 주는 이유는 학습속도를 비슷하게 맞추기위한 장치)</p>
</li>
<li>
<p>clipnorm의 사용: Actor-critic은 학습 안정성이 떨어져 optimizer가 update할때, 가중치의 크기가 커지는것을 막는역할.</p>
</li>
</ul>
<!--listend-->
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#60a0b0;font-style:italic"># 카트폴 예제에서의 액터-크리틱(A2C) 에이전트</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">class</span> <span style="color:#0e84b5;font-weight:bold">A2CAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">def</span> __init__(self, action_size):
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>render <span style="color:#666">=</span> <span style="color:#007020;font-weight:bold">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#60a0b0;font-style:italic"># 행동의 크기 정의</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>action_size <span style="color:#666">=</span> action_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#60a0b0;font-style:italic"># 액터-크리틱 하이퍼파라미터</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>discount_factor <span style="color:#666">=</span> <span style="color:#40a070">0.99</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>learning_rate <span style="color:#666">=</span> <span style="color:#40a070">0.001</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#60a0b0;font-style:italic"># 정책신경망과 가치신경망 생성</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>model <span style="color:#666">=</span> A2C(self<span style="color:#666">.</span>action_size)
</span></span><span style="display:flex;"><span>	<span style="color:#60a0b0;font-style:italic"># 최적화 알고리즘 설정, 미분값이 너무 커지는 현상을 막기 위해 clipnorm 설정</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>optimizer <span style="color:#666">=</span> Adam(lr<span style="color:#666">=</span>self<span style="color:#666">.</span>learning_rate, clipnorm<span style="color:#666">=</span><span style="color:#40a070">5.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#60a0b0;font-style:italic"># 정책신경망의 출력을 받아 확률적으로 행동을 선택</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">get_action</span>(self, state):
</span></span><span style="display:flex;"><span>	policy, _ <span style="color:#666">=</span> self<span style="color:#666">.</span>model(state)
</span></span><span style="display:flex;"><span>	policy <span style="color:#666">=</span> np<span style="color:#666">.</span>array(policy[<span style="color:#40a070">0</span>])
</span></span><span style="display:flex;"><span>	<span style="color:#007020;font-weight:bold">return</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>choice(self<span style="color:#666">.</span>action_size, <span style="color:#40a070">1</span>, p<span style="color:#666">=</span>policy)[<span style="color:#40a070">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#60a0b0;font-style:italic"># 각 타임스텝마다 정책신경망과 가치신경망을 업데이트</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">train_model</span>(self, state, action, reward, next_state, done):
</span></span><span style="display:flex;"><span>	model_params <span style="color:#666">=</span> self<span style="color:#666">.</span>model<span style="color:#666">.</span>trainable_variables
</span></span><span style="display:flex;"><span>	<span style="color:#007020;font-weight:bold">with</span> tf<span style="color:#666">.</span>GradientTape() <span style="color:#007020;font-weight:bold">as</span> tape:
</span></span><span style="display:flex;"><span>	    policy, value <span style="color:#666">=</span> self<span style="color:#666">.</span>model(state)
</span></span><span style="display:flex;"><span>	    _, next_value <span style="color:#666">=</span> self<span style="color:#666">.</span>model(next_state)
</span></span><span style="display:flex;"><span>	    target <span style="color:#666">=</span> reward <span style="color:#666">+</span> (<span style="color:#40a070">1</span> <span style="color:#666">-</span> done) <span style="color:#666">*</span> self<span style="color:#666">.</span>discount_factor <span style="color:#666">*</span> next_value[<span style="color:#40a070">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#60a0b0;font-style:italic"># 정책 신경망 오류 함수 구하기</span>
</span></span><span style="display:flex;"><span>	    one_hot_action <span style="color:#666">=</span> tf<span style="color:#666">.</span>one_hot([action], self<span style="color:#666">.</span>action_size)
</span></span><span style="display:flex;"><span>	    action_prob <span style="color:#666">=</span> tf<span style="color:#666">.</span>reduce_sum(one_hot_action <span style="color:#666">*</span> policy, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>)
</span></span><span style="display:flex;"><span>	    cross_entropy <span style="color:#666">=</span> <span style="color:#666">-</span> tf<span style="color:#666">.</span>math<span style="color:#666">.</span>log(action_prob <span style="color:#666">+</span> <span style="color:#40a070">1e-5</span>)
</span></span><span style="display:flex;"><span>	    advantage <span style="color:#666">=</span> tf<span style="color:#666">.</span>stop_gradient(target <span style="color:#666">-</span> value[<span style="color:#40a070">0</span>])
</span></span><span style="display:flex;"><span>	    actor_loss <span style="color:#666">=</span> tf<span style="color:#666">.</span>reduce_mean(cross_entropy <span style="color:#666">*</span> advantage)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#60a0b0;font-style:italic"># 가치 신경망 오류 함수 구하기</span>
</span></span><span style="display:flex;"><span>	    critic_loss <span style="color:#666">=</span> <span style="color:#40a070">0.5</span> <span style="color:#666">*</span> tf<span style="color:#666">.</span>square(tf<span style="color:#666">.</span>stop_gradient(target) <span style="color:#666">-</span> value[<span style="color:#40a070">0</span>])
</span></span><span style="display:flex;"><span>	    critic_loss <span style="color:#666">=</span> tf<span style="color:#666">.</span>reduce_mean(critic_loss)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	    <span style="color:#60a0b0;font-style:italic"># 하나의 오류 함수로 만들기</span>
</span></span><span style="display:flex;"><span>	    loss <span style="color:#666">=</span> <span style="color:#40a070">0.2</span> <span style="color:#666">*</span> actor_loss <span style="color:#666">+</span> critic_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#60a0b0;font-style:italic"># 오류함수를 줄이는 방향으로 모델 업데이트</span>
</span></span><span style="display:flex;"><span>	grads <span style="color:#666">=</span> tape<span style="color:#666">.</span>gradient(loss, model_params)
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>optimizer<span style="color:#666">.</span>apply_gradients(<span style="color:#007020">zip</span>(grads, model_params))
</span></span><span style="display:flex;"><span>	<span style="color:#007020;font-weight:bold">return</span> np<span style="color:#666">.</span>array(loss)
</span></span></code></pre></div><h4 id="agent의-model을-keras로-생성">Agent의 model을 keras로 생성</h4>
<ul>
<li>정책신경망(actor)와 가치신경망(critic)을 만든다.</li>
<li>actor</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/actor_network.png" width="400px"/>
</figure>

<ul>
<li>critic</li>
</ul>
<p><a id="figure--"></a></p>
<figure><img src="./img/chapter6/critic_network.png" width="400px"/>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#60a0b0;font-style:italic"># 정책 신경망과 가치 신경망 생성</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">class</span> <span style="color:#0e84b5;font-weight:bold">A2C</span>(tf<span style="color:#666">.</span>keras<span style="color:#666">.</span>Model):
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">def</span> __init__(self, action_size):
</span></span><span style="display:flex;"><span>	<span style="color:#007020">super</span>(A2C, self)<span style="color:#666">.</span>__init__()
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>actor_fc <span style="color:#666">=</span> Dense(<span style="color:#40a070">24</span>, activation<span style="color:#666">=</span><span style="color:#4070a0">&#39;tanh&#39;</span>)
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>actor_out <span style="color:#666">=</span> Dense(action_size, activation<span style="color:#666">=</span><span style="color:#4070a0">&#39;softmax&#39;</span>,
</span></span><span style="display:flex;"><span>			       kernel_initializer<span style="color:#666">=</span>RandomUniform(<span style="color:#666">-</span><span style="color:#40a070">1e-3</span>, <span style="color:#40a070">1e-3</span>))
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>critic_fc1 <span style="color:#666">=</span> Dense(<span style="color:#40a070">24</span>, activation<span style="color:#666">=</span><span style="color:#4070a0">&#39;tanh&#39;</span>)
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>critic_fc2 <span style="color:#666">=</span> Dense(<span style="color:#40a070">24</span>, activation<span style="color:#666">=</span><span style="color:#4070a0">&#39;tanh&#39;</span>)
</span></span><span style="display:flex;"><span>	self<span style="color:#666">.</span>critic_out <span style="color:#666">=</span> Dense(<span style="color:#40a070">1</span>,
</span></span><span style="display:flex;"><span>				kernel_initializer<span style="color:#666">=</span>RandomUniform(<span style="color:#666">-</span><span style="color:#40a070">1e-3</span>, <span style="color:#40a070">1e-3</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">call</span>(self, x):
</span></span><span style="display:flex;"><span>	actor_x <span style="color:#666">=</span> self<span style="color:#666">.</span>actor_fc(x)
</span></span><span style="display:flex;"><span>	policy <span style="color:#666">=</span> self<span style="color:#666">.</span>actor_out(actor_x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	critic_x <span style="color:#666">=</span> self<span style="color:#666">.</span>critic_fc1(x)
</span></span><span style="display:flex;"><span>	critic_x <span style="color:#666">=</span> self<span style="color:#666">.</span>critic_fc2(critic_x)
</span></span><span style="display:flex;"><span>	value <span style="color:#666">=</span> self<span style="color:#666">.</span>critic_out(critic_x)
</span></span><span style="display:flex;"><span>	<span style="color:#007020;font-weight:bold">return</span> policy, value
</span></span></code></pre></div><h3 id="open-a-dot-i-gym-simple-test">open a.i gym simple test</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">sys</span>
</span></span><span style="display:flex;"><span>sys<span style="color:#666">.</span>path<span style="color:#666">.</span>append(<span style="color:#4070a0">&#39;/Users/holy/opt/anaconda3/envs/keras_rl_3.7/lib/python3.7/site-packages&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">gym</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">pylab</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">random</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#666">=</span> gym<span style="color:#666">.</span>make(<span style="color:#4070a0">&#39;FrozneLake-v0&#39;</span>, is_slippery<span style="color:#666">=</span><span style="color:#007020;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>state <span style="color:#666">=</span> env<span style="color:#666">.</span>reset()
</span></span><span style="display:flex;"><span>env<span style="color:#666">.</span>render()
</span></span></code></pre></div><h3 id="open-a-dot-i-gym-cartpole-test">open a.i gym cartpole test</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">gym</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#666">=</span> gym<span style="color:#666">.</span>make(<span style="color:#4070a0">&#39;CartPole-v0&#39;</span>)
</span></span><span style="display:flex;"><span>env<span style="color:#666">.</span>reset()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">for</span> _ <span style="color:#007020;font-weight:bold">in</span> <span style="color:#007020">range</span>(<span style="color:#40a070">1000</span>):
</span></span><span style="display:flex;"><span>    env<span style="color:#666">.</span>render()
</span></span><span style="display:flex;"><span>    env<span style="color:#666">.</span>step(env<span style="color:#666">.</span>action_space<span style="color:#666">.</span>sample())     <span style="color:#60a0b0;font-style:italic"># Take a random action</span>
</span></span><span style="display:flex;"><span>env<span style="color:#666">.</span>close()
</span></span></code></pre></div>      
      </div>
        
  
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
  

  

  </div>
</div>  

<script src="/js/URI.js" type="text/javascript"></script>
<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
